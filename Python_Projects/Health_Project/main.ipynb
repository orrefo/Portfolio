{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting national health from subjectiv measure on a global scale\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from sklearn import linear_model\n",
    "from scipy.stats import pearsonr\n",
    "pd.options.display.max_columns=None\n",
    "pd.options.display.max_rows=150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "queastions=pd.read_csv(\"data/question_table.csv\")\n",
    "data_individual=pd.read_csv(\"data/data_individual_lvl.csv\")\n",
    "data_country=pd.read_csv(\"data/data_country_lvl.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# presentation \n",
    "How could we try to estimate the weelbeing of different nationas? well a good starting point is to ask! i invite you to follow along on an journey of exploration, narreted by me :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### World happiness report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the methodologcly for world happiness report(whr).\n",
    "\n",
    "\"Life evaluations from the Gallup World Poll provide the basis for the annual happiness rankings. They are based on answers to the main life evaluation question. The Cantril Ladder asks respondents to think of a ladder, with the best possible life for them being a 10 and the worst possible life being a 0. They are then asked to rate their own current lives on that 0 to 10 scale. The rankings are from nationally representative samples over three years.\"\n",
    "\n",
    "From world happiness report website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### World Values Survey\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is the methology of world Values Survey(wvs).\n",
    "\n",
    "\"The World Values Survey (www.worldvaluessurvey.org) is a global network of social scientists studying changing values and their impact on social and political life, led by an international team of scholars, with the WVS association and secretariat headquartered in Stockholm, Sweden. \n",
    "\n",
    "The survey, which started in 1981, seeks to use the most rigorous, high-quality research designs in each country. The WVS consists of nationally representative surveys conducted in almost 100 countries which contain almost 90 percent of the world’s population, using a common questionnaire. The WVS is the largest non-commercial, cross-national, time series investigation of human beliefs and values ever executed, currently including interviews with almost 400,000 respondents. Moreover the WVS is the only academic study covering the full range of global variations, from very poor to very rich countries, in all of the world’s major cultural zones.\n",
    "\n",
    "The WVS seeks to help scientists and policy makers understand changes in the beliefs, values and motivations of people throughout the world\"\n",
    "\n",
    "\n",
    "From world values survey website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rank comperison \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating own rank\n",
    "in order to establish a rank i first need to consider what metric i think will be of intrest. after looking through WVS almost 1000 different variables and 450 page variable explonation i selected 3 that i find good. it was \n",
    "1.  Feeling of happiness, 'A009'\n",
    "2.  Satisfaction with your life, 'A170'\n",
    "3.  State of health (subjective), 'A008'\n",
    "\n",
    "oskars_rank_with_health: is all three question, even perceived health.\n",
    "\n",
    "oskars_rank: is just the two first question, excluding percevied health, as this could take into account physical health and nut just mental health\n",
    "\n",
    "\n",
    "In order to compare against eachother and other variables i have both inverted and standardize neacessary columns values. \n",
    "\n",
    "    for columns in df.columns: \n",
    "        if columns in (list_of_columns_inverting):\n",
    "            max=df[columns].max()\n",
    "            test2=(df[columns]*-1)+max+1 #invert the values\n",
    "            df[columns]=test2+(test2-1)*((10-max)/(max-1)) #normalize values to 1-10 from x\n",
    "\n",
    "\n",
    "for futher details see creating_table_one_process where all relevant df and list are listed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_country['oskars_rank_with_health']=data_country[['A009','A008','A170']].sum(axis=1).rank(ascending=False)\n",
    "data_country['oskars_score_with_health']=data_country[['A009','A008','A170']].sum(axis=1)/3\n",
    "data_country['oskars_rank']=data_country[['A009','A170']].sum(axis=1).rank(ascending=False)\n",
    "data_country['oskars_score']=data_country[['A009','A170']].sum(axis=1)/2\n",
    "data_country['life_satisfaction_rank']=data_country['A170'].rank(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rank=data_country[['oskars_rank_with_health','oskars_rank','country_name','world_happiness_report_rank','life_satisfaction_rank']].sort_values(by='world_happiness_report_rank')\n",
    "data_rank['diff']=data_rank['oskars_rank_with_health']-data_rank['world_happiness_report_rank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*note that some of the outliers can be because of error in data and when the different data was collected. an example of this is israel, where the data doesn't exist for one of the variables('A009'). an fix for this and to in crease the robustness of my analysis would be to have avg of the three different columns instead of having a static sum/3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plotting the data  as scatter plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try plotting it as a scatter to try to see if there is a correlation between them :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rank.plot(x='oskars_rank_with_health',y='world_happiness_report_rank',kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "okey that looks bad, like zero correlation :/ \n",
    "\n",
    "so instead of doing rank lets try the raw score to see if there is a correlation:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_country.plot(x='oskars_score_with_health',y='Happiness score',kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "okey that looks a lot better. forewards lets use the score instead of the rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correlation analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets introduce the metrics of health that we have. i have choosen to focus on 4 different.\n",
    "1. Dailys_per_100000: It's the amount of year lost to disablility per 100000 from non communicable disieases in a country. it's a rough estimate of the health of the country.\n",
    "2. deppresion_percent: it's the precentage of the popluation with clinical deppresion, is a measure of \"unhappiness\"\n",
    "3. crude_suicide_per_100,000: it's the suicide rate per year per 100000 people. it's also a measure of \"extreme stress\" and \"unhappiness\"\n",
    "4. Life expectancy: it's the amount of year a person is expected to live, a measure of health.\n",
    "\n",
    "in order to do analysis i slim down the dataset and do correlation on that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "slim_down_list=['Happiness score',\n",
    "    'oskars_score_with_health',\n",
    "    'oskars_score',\n",
    "    'A009',\n",
    "    'A008',\n",
    "    'A170',\n",
    "    'A173',\n",
    "    'C006',\n",
    "    'H001',\n",
    "    'Unemployment rate',\n",
    "    'Dailys_per_100000',\n",
    "    'deppresion_percent',\n",
    "    'crude_suicide_per_100,000',\n",
    "    'Life expectancy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_slim=data_country[slim_down_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(queastions[['question','description']].loc[queastions['question'].isin(slim_down_list)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " okey let's do some analysis. it's a reagular correlation matrix and i have added stars* to mark a significant value. the values are 95%*(one star), 99%**(two stars) and 99.9%***(three stars) sure of an actual correlation. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = data_slim.corr()\n",
    "pval = data_slim.corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)\n",
    "p = pval.map(lambda x: ''.join(['*' for t in [.05, .01, .001] if x<=t]))\n",
    "rho.round(2).astype(str) + p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intressting, there are some very strong correlations here. fore example is happiness score a stronbg correlation with both Life expectancy and dalys. but they also correlate psoitive with suicide, so the mopre happy a country is the more suicides happen? that doesnt sound right. if we look at my score then it's actually signifiklant negative correlation with suicide, that's a good sign. let's follow to the next and final step, linear regression. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def stepwise linearRegression alghoritm\n",
    "\n",
    "This part is where i define the the algorithm for selecting variables for the regression analysis, with a stepwise approach. what this does is it loops over a model with all variables and remove the one with the highest pvalue. then it reruns the model and removes the next one with the highest p value. it does this untill it reaches a user defined pvalue. i will use a standard 0.05 for pvalue. i have multiple different models with sligthly different outpout and amount of prints. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wvs_corr=['A001',\n",
    "    'A002', \n",
    "    'A003', \n",
    "    'A004', \n",
    "    'A005',\n",
    "    'A006',\n",
    "    'A008', \n",
    "    'A009', \n",
    "    'A170',\n",
    "    'A173', \n",
    "    'C006', \n",
    "    'E035',\n",
    "    'E124',  \n",
    "    'F028', \n",
    "    'F063', \n",
    "    'G006',\n",
    "    'X011',\n",
    "    'X047_WVS',\n",
    "    'Y001',\n",
    "    'Y002',\n",
    "    'Y003',\n",
    "    'Y010',\n",
    "    'Y011',\n",
    "    'Y011A',\n",
    "    'Y011B',\n",
    "    'Y011C',\n",
    "    'Y012',\n",
    "    'Y012A',\n",
    "    'Y012B',\n",
    "    'Y012C',\n",
    "    'Y013',\n",
    "    'Y013A',\n",
    "    'Y013B',\n",
    "    'Y013C',\n",
    "    'Y014',\n",
    "    'Y014A',\n",
    "    'Y014B',\n",
    "    'Y020',\n",
    "    'Y021',\n",
    "    'Y021A',\n",
    "    'Y021B',\n",
    "    'Y021C',\n",
    "    'Y022',\n",
    "    'Y022A',\n",
    "    'Y022B',\n",
    "    'Y022C',\n",
    "    'Y023',\n",
    "    'Y023A',\n",
    "    'Y023B',\n",
    "    'Y023C',\n",
    "    'Y024',\n",
    "    'Y024A',\n",
    "    'Y024B',\n",
    "    'Y024C',\n",
    "    'X045_val',\n",
    "    'Happiness score',\n",
    "    'oskars_score_with_health',\n",
    "    'oskars_score']\n",
    "wvs_corr_measure=['Dailys_per_100000',\n",
    "    'deppresion_percent',\n",
    "    'crude_suicide_per_100,000',\n",
    "    'Life expectancy']\n",
    "wvs_tot=wvs_corr+wvs_corr_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepwise_regression(value_column,p_value):\n",
    "    wvs_corr_temp=wvs_corr.copy()\n",
    "    wvs_tot_temp=wvs_tot.copy()\n",
    "    iteration=0\n",
    "    data_model=data_country[wvs_tot_temp].dropna()\n",
    "    x = data_model[wvs_corr_temp]\n",
    "    y = data_model[value_column]\n",
    "    results = sm.GLS(y, x).fit()\n",
    "    print(results.summary())\n",
    "    pvalues=results.pvalues\n",
    "    while pvalues.max()>p_value:    \n",
    "        data_pval=pd.DataFrame(pvalues,columns=['pval'])\n",
    "        print(f\"r-squared {round(results.rsquared,3)} for iteration {iteration} with highest pvalue remaning {round(pvalues.max(),3)}, removing {pvalues.idxmax()}\")\n",
    "        wvs_tot_temp.remove(data_pval['pval'].idxmax())\n",
    "        data_pval=data_pval.drop(data_pval['pval'].idxmax())\n",
    "        data_model=data_country[wvs_tot_temp].dropna()\n",
    "        x = data_model[data_pval.index]\n",
    "        y = data_model[value_column]\n",
    "        results = sm.GLS(y, x).fit()\n",
    "        iteration+=1\n",
    "        pvalues=results.pvalues\n",
    "    print(results.summary())\n",
    "    return(data_pval.index)\n",
    "\n",
    "def stepwise_regression_slim(value_column,p_value):\n",
    "    wvs_corr_temp=wvs_corr.copy()\n",
    "    wvs_tot_temp=wvs_tot.copy()\n",
    "    data_model=data_country[wvs_tot_temp].dropna()\n",
    "    x = data_model[wvs_corr_temp]\n",
    "    y = data_model[value_column]\n",
    "    results = sm.GLS(y, x).fit()\n",
    "    pvalues=results.pvalues\n",
    "    while pvalues.max()>p_value:    \n",
    "        data_pval=pd.DataFrame(pvalues,columns=['pval'])\n",
    "        wvs_tot_temp.remove(data_pval['pval'].idxmax())\n",
    "        data_pval=data_pval.drop(data_pval['pval'].idxmax())\n",
    "        data_model=data_country[wvs_tot_temp].dropna()\n",
    "        x = data_model[data_pval.index]\n",
    "        y = data_model[value_column]\n",
    "        results = sm.GLS(y, x).fit()\n",
    "        pvalues=results.pvalues\n",
    "    print(results.summary())\n",
    "    return(data_pval.index)\n",
    "\n",
    "def stepwise_regression_slim_list(value_column,p_value):\n",
    "    wvs_corr_temp=wvs_corr.copy()\n",
    "    wvs_tot_temp=wvs_tot.copy()\n",
    "    data_model=data_country[wvs_tot_temp].dropna()\n",
    "    x = data_model[wvs_corr_temp]\n",
    "    y = data_model[value_column]\n",
    "    results = sm.GLS(y, x).fit()\n",
    "    pvalues=results.pvalues\n",
    "    while pvalues.max()>p_value:    \n",
    "        data_pval=pd.DataFrame(pvalues,columns=['pval'])\n",
    "        wvs_tot_temp.remove(data_pval['pval'].idxmax())\n",
    "        data_pval=data_pval.drop(data_pval['pval'].idxmax())\n",
    "        data_model=data_country[wvs_tot_temp].dropna()\n",
    "        x = data_model[data_pval.index]\n",
    "        y = data_model[value_column]\n",
    "        results = sm.GLS(y, x).fit()\n",
    "        pvalues=results.pvalues\n",
    "    return(data_pval.index)\n",
    "    \n",
    "\n",
    "def stepwise_regression_slim_list(value_column,p_value):\n",
    "    wvs_corr_temp=wvs_corr.copy()\n",
    "    wvs_tot_temp=wvs_tot.copy()\n",
    "    data_model=data_country[wvs_tot_temp].dropna()\n",
    "    x = data_model[wvs_corr_temp]\n",
    "    y = data_model[value_column]\n",
    "    results = sm.GLS(y, x).fit()\n",
    "    pvalues=results.pvalues\n",
    "    while pvalues.max()>p_value:    \n",
    "        data_pval=pd.DataFrame(pvalues,columns=['pval'])\n",
    "        wvs_tot_temp.remove(data_pval['pval'].idxmax())\n",
    "        data_pval=data_pval.drop(data_pval['pval'].idxmax())\n",
    "        data_model=data_country[wvs_tot_temp].dropna()\n",
    "        x = data_model[data_pval.index]\n",
    "        y = data_model[value_column]\n",
    "        results = sm.GLS(y, x).fit()\n",
    "        pvalues=results.pvalues\n",
    "    return(data_pval.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def the singe variable linear regresion\n",
    "\n",
    "this part i define the function where i take a single variable and do a linear analysis. here there is also a mangitud of different variations of the original function with different outpouts. a differance for the original big model is that i have included models with a constant, which does a big differance in the model. i think that all models should have constant, but i didn't get it to play nice with thew big model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_variable_linear_score(variable):\n",
    "    score=[]\n",
    "    for values in wvs_corr_measure:\n",
    "        slim=[]\n",
    "        slim.append(values)\n",
    "        slim.append(variable)\n",
    "        data_model=data_country[slim].dropna()\n",
    "        x = data_model[[variable]]\n",
    "        y = data_model[values]\n",
    "        results = sm.GLS(y, x).fit()\n",
    "        score.append(results.tvalues[variable])\n",
    "    return(score)\n",
    "\n",
    "def one_variable_linear_score_constant(variable):\n",
    "    score=[]\n",
    "    for values in wvs_corr_measure:\n",
    "        slim=[]\n",
    "        slim.append(values)\n",
    "        slim.append(variable)\n",
    "        data_model=data_country[slim].dropna()\n",
    "        x = data_model[[variable]]\n",
    "        y = data_model[values]\n",
    "        x= sm.add_constant(x)\n",
    "        results = sm.GLS(y, x).fit()\n",
    "        score.append(results.tvalues[variable])\n",
    "    return(score)\n",
    "\n",
    "def one_variable_linear_regression(variable):\n",
    "    slim=[]\n",
    "    for values in wvs_corr_measure:\n",
    "        slim=[]\n",
    "        slim.append(values)\n",
    "        slim.append(variable)\n",
    "        data_model=data_country[slim].dropna()\n",
    "        x = data_model[[variable]]\n",
    "        y = data_model[values]\n",
    "        reg = linear_model.LinearRegression()\n",
    "        reg.fit(x, y)\n",
    "        results = sm.GLS(y, x).fit()\n",
    "        print(results.summary())\n",
    "        print(reg.score(x, y))\n",
    "\n",
    "def one_variable_linear_regression_with_constant(variable):\n",
    "    slim=[]\n",
    "    for values in wvs_corr_measure:\n",
    "        slim=[]\n",
    "        slim.append(values)\n",
    "        slim.append(variable)\n",
    "        data_model=data_country[slim].dropna()\n",
    "        x = data_model[[variable]]\n",
    "        y = data_model[values]\n",
    "        x= sm.add_constant(x)\n",
    "        reg = linear_model.LinearRegression()\n",
    "        reg.fit(x, y)\n",
    "        results = sm.GLS(y, x).fit()\n",
    "        print(results.summary())\n",
    "        print(reg.score(x, y))\n",
    "\n",
    "def one_variable_linear_score_comparision(variable):\n",
    "    score=[]\n",
    "    for values in wvs_corr_measure:\n",
    "        slim=[]\n",
    "        slim.append(values)\n",
    "        slim.append(variable)\n",
    "        data_model=data_country[slim].dropna()\n",
    "        x = data_model[[variable]]\n",
    "        y = data_model[values]\n",
    "        reg = linear_model.LinearRegression()\n",
    "        reg.fit(x, y)\n",
    "        score.append(reg.score(x, y))\n",
    "    return(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perfomring analysis with reggresion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first we perfom a stepvis analysis to figure out the best way to construct our linear regression model. first i do a single dependent variable and print out all steps along the way, showing initial model and all the trim down and the final model.    :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepwise_regression('deppresion_percent',0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that is a lot of text and outprint, so binstead of showing every step let's just print out the final model and how good it is to predict the actual score. additionaly i print out the description to the last model. i cycle through the 4 metrics and printy out the model for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_r_values=[]\n",
    "for values in wvs_corr_measure:\n",
    "    stepwise_regression_slim(values,0.05)\n",
    "    temp_list=stepwise_regression_slim_list(values,0.05).to_list()\n",
    "    slim=temp_list+wvs_corr_measure\n",
    "    data_model=data_country[slim].dropna()\n",
    "    x = data_model[temp_list]\n",
    "    y = data_model[values]\n",
    "    reg = linear_model.LinearRegression()\n",
    "    reg.fit(x, y)\n",
    "    print(f\"the score value is {round(reg.score(x, y),3)}\")\n",
    "    list_r_values.append(reg.score(x, y))\n",
    "    print(queastions[['question','description']].loc[queastions['question'].isin(temp_list)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nice, seams like we have predictrive values from the models. ranging from 0.236-0.818 is a wide span, but are they good? lets compare how well they do compared to single variable. let's create a big table for every single variable and rank how well they predict each dependent variable. let's show it of with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_variable_linear_regression('oskars_score_with_health')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that was truly bad score, let's run thrpough the entierty of the variables and se how each performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(columns=['variable','Dailys_per_100000',\n",
    " 'deppresion_percent',\n",
    " 'crude_suicide_per_100,000',\n",
    " 'Life expectancy'])\n",
    "\n",
    "for value in wvs_corr:\n",
    "    list=[value]\n",
    "    list.extend(one_variable_linear_score_comparision(value))\n",
    "    df=pd.concat([df,pd.DataFrame([list], columns=['variable','Dailys_per_100000','deppresion_percent','crude_suicide_per_100,000','Life expectancy'])], ignore_index=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and let's add the regresion model, i saved a list from the original analysis so let's add that on :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "val=['regresion_model']\n",
    "val.extend(list_r_values)\n",
    "df=pd.concat([df,pd.DataFrame([val], columns=['variable','Dailys_per_100000','deppresion_percent','crude_suicide_per_100,000','Life expectancy'])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so the best predictor fopr dalys is the regression model, iwth a 0.27 point increase from the second. and we can se that my score is way down the list, so it wasn't a good predictor of dalys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='Dailys_per_100000',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and for suicide it is a good increase compared to other measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='crude_suicide_per_100,000',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so that's where my time and work is done, thank you for following along on this path. hope i have inspired azsome thought. and i must say the happiness rank from World Happiness Report is a very god predictor for most mental health indicator, although it's very bad on one particular thing, suicide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extra stuff about my limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first, there a liute note about the correlation matrix not being defined. i think that my propibly is wrong since i haven't provided one. secondly is that i haven't included a constant in my models. below this model is additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_variable_linear_regression_with_constant('Happiness score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we see addding a constant which is best practice will significantly decreese the effect on the variable. i think i would really need this for the multiple variable analysis but i cant figure out how. where is the t value for each varaible, where a significant correlation is below -2 and above +2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_constant=pd.DataFrame(columns=['variable','Dailys_per_100000',\n",
    " 'deppresion_percent',\n",
    " 'crude_suicide_per_100,000',\n",
    " 'Life expectancy'])\n",
    "\n",
    "for value in wvs_corr:\n",
    "    list=[value]\n",
    "    list.extend(one_variable_linear_score_constant(value))\n",
    "    df_constant=pd.concat([df_constant,pd.DataFrame([list], columns=['variable','Dailys_per_100000','deppresion_percent','crude_suicide_per_100,000','Life expectancy'])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there is also a lot of extra data that i did not have the time to analys, almost all of the demografic data. i think that i have taken on a bigger bite then i could chew, a bit to optimistic with time and ability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
